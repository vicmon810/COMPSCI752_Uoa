{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b65d348-858f-49f7-804c-42647049c1a0",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "Consider the collection of 4 pre-processed documents. Each document is represented as a list of terms together with its weight as follows:\n",
    "    d1 {(andrew, 6), (teach, 4), (logic, 2), (database, 5)}\n",
    "    d2 {(peter, 6), (study, 2), (python, 6), (logic, 3)}\n",
    "    d3 {(andrew, 5), (teach, 1), (python, 5)}\n",
    "    d4 {(peter, 5), (study, 4), (like, 2), (python, 5)}\n",
    "We represent each document as a high-dimensional vector and use the inner product as the similarity measure. We will use the prefix-filtering algorithm to find similar pairs of documents whose inner product values are larger than a threshold t.\n",
    "1. Compute the similarity of each pair of non-identical documents. [5 marks]\n",
    "    \n",
    "\n",
    "inner product : \n",
    "\n",
    "    - <d1,d2>: 2 * 3  = 6\n",
    "    - <d1,d3>: 5 * 6 + 1 * 4 = 30 + 4 = 34\n",
    "    - <d1,d4>: 0 \n",
    "    - <d2,d3>: 5 * 6 = 30\n",
    "    - <d2,d4>: 5 * 6 + 2 * 4 + 5 * 6 = 30 +  8 + 30 = 68\n",
    "    - <d3,d4>: 5 * 5 = 25 \n",
    "\n",
    "summary : \n",
    "\n",
    "    similarity d1 and d2 : 6\n",
    "    similarity d1 and d3 : 34\n",
    "    similarity d1 and d4 : 0\n",
    "    similarity d2 and d3 : 30\n",
    "    similarity d2 and d4 : 68\n",
    "    similarity d3 and d4 : 25\n",
    "    \n",
    "\n",
    "\n",
    "2. Assume that the dimension indexes are as follows:<br>\n",
    "    {(andrew, 1), (peter, 2), (teach, 3), (study, 4), (like, 5),(logic, 6), (database, 7), (python, 8)}.<br>\n",
    "    Construct the signature of each document given the threshold t = 37. What are the candidate pairs that we need to compute the similarity? [10 marks]\n",
    "\n",
    "data setup:\n",
    "\n",
    "        d1  {(andrew, 6), (teach, 4), (logic,2), (database, 5)} \n",
    "        d2  {(peter,6), (study, 2), (logic, 3), (python, 6)}\n",
    "        d3  {(andrew,5), (teach,1), (python,5)}\n",
    "        d4  {(peter,5),(study,4), (like,2), (python, 5) }\n",
    "\n",
    "\n",
    "dim : {(andrew, 1), (peter, 2), (teach, 3), (study, 4), (like, 5),(logic, 6), (database, 7), (python, 8)}\n",
    "\n",
    "t = 37\n",
    "\n",
    "p(d1): \n",
    "    - 6 * 6 = 36 < t\n",
    "  \n",
    "    - 6 * 6 + 4 * 4 = 52 > t\n",
    "  \n",
    "S(d1): {(teach,4), (logic,2). (database,5)}\n",
    "\n",
    "P(d2): \n",
    "    - 6 * 6 = 36 < t\n",
    "  \n",
    "    - 6 * 6 + 2 * 4 = 44 > t\n",
    "  \n",
    "S(d2):  {(study, 2), (logic, 3), (python, 6)}\n",
    "\n",
    "p(d3):\n",
    "    - 5 * 6 = 30  < t\n",
    "  \n",
    "    - 5 * 6 + 1 * 4 = 34 < t\n",
    "  \n",
    "    - 5 * 6 + 1 * 4 + 5 * 6 = 64 > t\n",
    "  \n",
    "S(d3): {(python,5)}\n",
    "\n",
    "\n",
    "p(d4): \n",
    "    - 5 * 6 = 30 < t\n",
    "    - 5 * 6 + 4 * 4 = 46 > t\n",
    "  \n",
    "S(d4): {(study,4), (like,2), (python, 5) }\n",
    "\n",
    "\n",
    "\n",
    "S(d1): {(teach,4), (logic,2). (database,5)}\n",
    "\n",
    "S(d2):  {(study, 2), (logic, 3), (python, 6)}\n",
    "\n",
    "S(d3): {(python,5)}\n",
    "\n",
    "S(d4): {(study,4), (like,2), (python, 5) }\n",
    "\n",
    "\n",
    "d1 and d2 share : {(logic)}\n",
    "\n",
    "d1 and d3 share : {}\n",
    "\n",
    "d1 and d4 sahre : {}\n",
    "\n",
    "d2 and d3 share : {(python)}\n",
    "\n",
    "d2 and d4 share : {(study), (python)}\n",
    "\n",
    "d3 and d4 share : {(python)}\n",
    "\n",
    "Candidate pair:  (d1,d2), (d2,d3), (d2,d4), (d3,d4)\n",
    "\n",
    "Verification:\n",
    "\n",
    "<d1,d2>:  2 * 3 = 6 < t\n",
    "\n",
    "<d2,d3>: 6 * 5 = 30 < t\n",
    "\n",
    "<d2,d4>: 2 * 4 + 5 * 6 = 38 > t\n",
    "\n",
    "<d3,d4>: 5 * 5 = 25\n",
    "\n",
    "Final pair meets threshold: (d2,d4)\n",
    "\n",
    "3. There are several way to index the dimensions, for example {(peter, 1), (andrew, 2), (teach, 3), (study, 4), (like, 5), (logic, 6), (database, 7), (python, 8)}.\n",
    "Are there any way to index the dimensions so that we use less space to store our inverted index? Explain your solution. [5 mark]\n",
    "\n",
    "\n",
    " analyze the frequency of each term :\n",
    "\n",
    " andrew: 2 occurrences\n",
    "\n",
    "peter: 2 occurrences\n",
    "\n",
    "teach: 1 occurrence\n",
    "\n",
    "study: 2 occurrences\n",
    "\n",
    "like: 1 occurrence\n",
    "\n",
    "logic: 2 occurrences\n",
    "\n",
    "database: 1 occurrence\n",
    "\n",
    "python: 3 occurrences\n",
    "\n",
    "Based on the frequency analysis, we can assign smaller indexes to the terms with higher frequencies. This ensures that the terms with more occurrences are represented by smaller numbers, reducing the space required for the inverted index.\n",
    "\n",
    "\n",
    "assign indexes as follows:\n",
    "\n",
    "python: 1 (most frequent)\n",
    "\n",
    "andrew: 2\n",
    "\n",
    "peter: 3\n",
    "\n",
    "logic: 4\n",
    "\n",
    "study: 5\n",
    "\n",
    "teach: 6\n",
    "\n",
    "like: 7\n",
    "\n",
    "database: 8 (least frequent)\n",
    "\n",
    "This indexing technique guarantees that the most common phrase, \"python,\" has the least index, followed by subsequent terms depending on their frequency of occurrence.\n",
    "\n",
    "Using this indexing approach, we may encode the dimensions with fewer bits, resulting in a more compact inverted index and saving total storage space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c774c7",
   "metadata": {},
   "source": [
    " # Question 2\n",
    "\n",
    "We use the MapReduce version of the prefix-filtering algorithm to find all pairs of doc-\n",
    "uments whose inner product similarity is at least a threshold thres. Assume that we\n",
    "know the maximum vector vector m in advance, and use it to compute the signature of\n",
    "each document. For each document i, we maintain di containing (1) the document index\n",
    "i, and (2) a vector containing all terms and its corresponding weight. The MapReduce\n",
    "version of prefix-filtering algorithm is identical to the algorithm from the lecture, and is\n",
    "provided as follows.\n",
    "Algorithm 1 map(key: i, value: di)\n",
    "1: Construct the prefix-filtering signature S(di)\n",
    "2: for each term t in the signature S(di) do\n",
    "3: emit(key: t, value: di)\n",
    "4: end for\n",
    "Algorithm 2 reduce(key: t, value: D = [d0, . . . , dk], the list of k documents whose\n",
    "signature contains term t)\n",
    "1: for each document pair 〈i, j〉 where 0 ≤ i < j ≤ k in the list D do\n",
    "2: Compute the similarity σ(di, dj )\n",
    "3: if σ(di, dj ) ≥ thres then\n",
    "4: emit(key: 〈i, j〉, value: σ(di, dj ))\n",
    "5: end if\n",
    "6: end for\n",
    "1. Explain the condition where MapReduce prefix-filtering runs faster than the standard prefix-fitering algorithm. [5 marks]\n",
    "\n",
    "\n",
    "- Large Dataset with Distributed Processing\n",
    "  - when the dataset is large and can be spearat to small sgements, Mapreduce can performance effiency computation time than standard prefix-fitering algorithm.\n",
    "- High Dimensional Data\n",
    "  - When the document vectors have high dimensionality, the workload can be distributed across multiple nodes in the MapReduce framework, reducing the computational burden on any single node.\n",
    "- Efficient Term Distribution\n",
    "  - If the terms in the document vectors are well-distributed across the documents, the MapReduce framework can efficiently distribute the load among different reducers. This reduces the possibility of any single reducer becoming a bottleneck.\n",
    "- Network Bandwidth\n",
    "  - When the network bandwidth between nodes is sufficient to handle the communication overhead of shuffling data between the map and reduce phases, the distributed nature of MapReduce can outperform a centralized algorithm.\n",
    "\n",
    "Overall, MapReduce only runs faster than the standard prefix-filtering in a distributed database, otherwise Mapreduce have no significant outstanding than others. \n",
    "\n",
    "\n",
    "2. Explain the condition where MapReduce prefix-filtering runs as slow as the standard prefix-fitering algorithm. Present your solution to fix it. [10 marks]\n",
    "\n",
    "- Skewed Data Distribution:\n",
    "- Low Parallelism\n",
    "- High Communication Overhead\n",
    "- Insufficient Resources:\n",
    "\n",
    "How to fix : \n",
    "\n",
    "- Data Preprocessing and Balancing:Preprocess the data to balance the load among reducers. This can include techniques like partitioning the data based on term frequencies to ensure even distribution.\n",
    "- Combiner Function:Use a combiner function to reduce the amount of intermediate data before it is shuffled to the reducers. This can help in minimizing the communication overhead.\n",
    "- Efficient Network Utilization:Optimize network usage by compressing intermediate data before shuffling and using high-throughput network infrastructure to reduce latency and bandwidth issues.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529df3e",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "In our lecture, we have studied the reservoir sampling which samples an element from a stream of size m with the same probability. If we use the reservoir sampling with the\n",
    "summary size s = 1, each element of a stream will be sampled with probability 1/m. We name this method as RS1. The generalized version of reservoir sampling with the\n",
    "summary size s > 1 guarantees that each element in a stream will be sampled with the same probability s/m. We name this method as RSs.Figure 1: Illustration of how pRS1 works.\n",
    "In the assignment, we consider a new algorithm, called pRS1, that simulates RSs for\n",
    "s > 1 by running s independent RS1 instances in parallel. pRS1 also uses a summary\n",
    "of size s, as shown in Figure 1.\n",
    "\n",
    "\n",
    "1. As a function of m and s, what is the probability an element of a stream is sampled by pRS1?\n",
    "\n",
    "In RS1, each element from the stream is sampled with a probability of 1/m, where m is the size of the reservoir. Now, if we run s independent RS1 instances in parallel, the probability that an element is not sampled by any of these instances can be calculated as (1 - 1/m)^s. This is because for each instance, the probability of not sampling an element is (1 - 1/m), and since they are independent, we multiply these probabilities together.\n",
    "\n",
    "Therefore, the probability that an element is sampled by at least one of the RS1 instances (which is equivalent to being sampled by pRS1) is the complement of this, which is 1 - (1 - 1/m)^s.\n",
    "\n",
    "\n",
    "\n",
    "2. Let fi be the number of occurrences of the element ai in a stream. Explain how we can use RSs and pRS1 to estimate fi.\n",
    "\n",
    "\n",
    "We define a random variable Xj for each cell in the summary, representing whether the element ai appears at that cell or not. The expected value of Xj is fi/m because in RSs, each element is sampled with a probability s/m, and thus the expected number of occurrences of ai in the cell j is fi/m.\n",
    "\n",
    "Then, we define the random variable X as the sum of all Xj, representing the total number of occurrences of ai in the summary. The expected value of X is sfi/m because there are s cells in the summary, and each contributes fi/m on average. Therefore, to estimate fi, we divide Xm by s, as the total expected count is distributed evenly across the cells."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
